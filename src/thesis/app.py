import asyncio
import textwrap
import os
import sys
from uuid import uuid4
import gradio as gr
from langgraph.checkpoint.memory import InMemorySaver

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from llm.graph.build import create_memory_aware_graph  

def make_user_friendly(text: str) -> str:
    """Format text for a more readable display."""
    formatted = textwrap.dedent(text).strip()
    return formatted.replace("**", "").replace("\\n", "\n")


def run_app():
    THREAD_ID = str(uuid4())
    memory = InMemorySaver()
    graph = create_memory_aware_graph(checkpointer=memory)

    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue")) as demo:
        chat_state = gr.State([])  # [[(msg, response), ...]]
        history_state = gr.State([])  # conversation_history: list[dict]

        gr.Markdown("# ðŸ§  AI Product Assistant")
        gr.Markdown("Ask for information or recommendations about products.")

        chat_display = gr.Markdown()

        with gr.Row():
            msg = gr.Textbox(
                placeholder="Ask a question or request a recommendation...",
                scale=9,
                container=False,
                show_label=False
            )
            submit = gr.Button("Send", scale=1, variant="primary")

        with gr.Row():
            examples = gr.Examples(
                examples=[
                    "What's the difference between machine learning and deep learning?",
                    "Can you recommend a product for eyebrow under 10$ ?",
                    "Can you recommend me a fantasy book?",
                    "Who is Cristiano Ronaldo?"
                ],
                inputs=msg
            )

        def update_chat_display(history_pairs):
            markdown = ""
            for user_msg, bot_msg in history_pairs:
                markdown += f"### ðŸ‘¤ You\n{user_msg}\n\n"
                markdown += f"### ðŸ¤– Assistant\n{bot_msg}\n\n"
                markdown += "---\n\n"
            return markdown

        def process_message(message, history_pairs, conv_history):
            if not message:
                return "", history_pairs, conv_history, update_chat_display(history_pairs)

            initial_state = {
                "input_query": message,
                "thread_id": THREAD_ID,
                "conversation_history": conv_history,
                "answer": "",
                "label": None,
                "context_summary": None,
            }

            try:
                final_state = asyncio.run(graph.ainvoke(
                    initial_state,
                    config={"configurable": {"thread_id": THREAD_ID}}
                ))

                response = make_user_friendly(final_state.get("answer", "No response generated by the assistant."))
                updated_history = final_state.get("conversation_history", conv_history)

            except Exception as e:
                response = f"Error during processing: {str(e)}"
                updated_history = conv_history

            history_pairs.append((message, response))
            return "", history_pairs, updated_history, update_chat_display(history_pairs)

        submit_click = submit.click(
            process_message,
            [msg, chat_state, history_state],
            [msg, chat_state, history_state, chat_display]
        )

        msg_submit = msg.submit(
            process_message,
            [msg, chat_state, history_state],
            [msg, chat_state, history_state, chat_display]
        )

        with gr.Accordion("System Information", open=False):
            gr.Markdown("""
            **How it works:**
            1. BERT classifier analyzes your request.
            2. Based on the classification, one of these agents is activated:
               - Recommendation agent for product advice
               - General QA agent for general questions
            3. The response is generated and displayed.
            4. Previous messages are remembered to enrich context when needed.
            """)

    return demo


if __name__ == "__main__":
    demo = run_app()
    demo.launch(share=True)
