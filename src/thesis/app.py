import asyncio
import textwrap
import os
import sys
from uuid import uuid4
import gradio as gr
from langgraph.checkpoint.memory import InMemorySaver


sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
os.environ["TOKENIZERS_PARALLELISM"] = "false"


from llm.graph.build import create_graph

def make_user_friendly(text: str) -> str:
    """Format text for a more readable display."""
    formatted = textwrap.dedent(text).strip()
    return formatted.replace("**", "").replace("\\n", "\n")

def run_app():

    THREAD_ID = str(uuid4())

    memory = InMemorySaver()
    graph = create_graph(checkpointer=memory)
    

    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue")) as demo:

        chat_state = gr.State([])
        
        gr.Markdown("# ðŸ§  AI Product Assistant")
        gr.Markdown("Ask for information or recommendations about products.")

        chat_display = gr.Markdown()
        
        with gr.Row():
            msg = gr.Textbox(
                placeholder="Ask a question or request a recommendation...",
                scale=9,
                container=False,
                show_label=False
            )
            submit = gr.Button("Send", scale=1, variant="primary")

        with gr.Row():
            examples = gr.Examples(
                examples=[
                    "What's the difference between machine learning and deep learning?",
                    "Can you recommend a laptop for machine learning?",
                    "What are the main features of TensorFlow?",
                    "Recommend a good monitor for data science."
                ],
                inputs=msg
            )
        
        def update_chat_display(history):
            """Convert chat history to formatted markdown for display"""
            markdown = ""
            for i, (user_msg, bot_msg) in enumerate(history):
                markdown += f"### ðŸ‘¤ You\n{user_msg}\n\n"
                markdown += f"### ðŸ¤– Assistant\n{bot_msg}\n\n"
                markdown += "---\n\n"
            return markdown
        
        def process_message(message, history):
            """Process a message and update chat history"""
            if not message:
                return "", history, update_chat_display(history)

            initial_state = {
                "input_query": message,
                "thread_id": THREAD_ID,
            }
            
            try:
                final_state = asyncio.run(graph.ainvoke(
                    initial_state, 
                    config={"configurable": {"thread_id": THREAD_ID}}
                ))
                
                if "answer" in final_state:
                    response = make_user_friendly(final_state["answer"])
                else:
                    response = "No response generated by the assistant."
                    
            except Exception as e:
                response = f"Error during processing: {str(e)}"

            history.append((message, response))

            return "", history, update_chat_display(history)

        submit_click = submit.click(
            process_message,
            [msg, chat_state],
            [msg, chat_state, chat_display]
        )
        
        msg_submit = msg.submit(
            process_message,
            [msg, chat_state],
            [msg, chat_state, chat_display]
        )

        with gr.Accordion("System Information", open=False):
            gr.Markdown("""
            **How it works:**
            1. BERT classifier analyzes your request
            2. Based on the classification, one of these agents is activated:
               - Recommendation agent for product advice
               - General QA agent for generic questions
            3. The response is generated and displayed
            """)
    
    return demo

if __name__ == "__main__":
    demo = run_app()
    demo.launch(share=True)